需求：统计主站每个指定课程访问的客户端 地域信息的分布
地域 ip转换 spark sql项目实战
 useragent 获取  hadoop基础课程
离线的方式进行统计

实现步骤 
	课程编号 ip信息 useragent
	mapreduce 和spark
 项目架构
	日志收集 flume
	离线分析 mapreduce/spark
	统计结果图形化展示
 问题     小时级别   5分钟   1分钟 秒级别
mapreduce 的时效性不高 mapreduce 启动和销毁进程回浪费时间
离线的批处理 spark sparksql map/reduce
解决 ===> 实时流处理的框架
时效性高  业务营销
数据量大  以实时方式处理大的数据 保证数据的准确性 扩展性好的系统 生命周期短的业务 
从价值密度低的数据提取有价值数据 
实时计算  流式计算  实时流式计算
数据来源
	hdfs 历史数据 数据量比较大
	实时：消息队列（kafka）实时新增/修改记录过来的某一笔数据
处理过程：
	离线 map+reduce 慢


SparkStreaming
Sparkconf --> Streamingcontxt(conf, 切分批次)

	实时 spark（DStream/SS）
处理速度
	慢
	快速
进程
	离线： 启动销毁
	实时： 7×24



apache storm 真正的实时框架
apache spark streaming 微批处理 按照时间间隔缩小 1秒 2秒
IBM Strea
Yahoo S4
LinkedIn Kafka streaming platform
Flink

WEB/APP            WebServer(tomcat nginx)access.log      flume(分布式日志收集框架)
加kafka是为了抗住高峰时的日志压力       spark/storm          rdbms/nosql           可视化展示
实际应用 
电信行业流量计算的实时计算 防止流量超标 手机频繁网络传输 有流量陷阱的软件
电商行业 1.大屏幕展示 2.推测属于哪一个用户 分析出关联产品 





Flume
network divices	operating system	web services		Applications
其他的server移动到hadoop上
1.采用脚本直接传输 cp到集群的机器上 hadoop fs -put
   问题 没有办法做到很好的监控  实效性会低   文件直接传输io开销大  怎么做容错和负载均衡
   脚本回出现问题
2.Flume 
   Flume的使用主要是写配置文件  从A->B
  分布式 高可靠  
  collect收集 	aggreating聚合		moving 移动
 在线实时的分析应用 指定webserver 从webserver
  source + channel + sink
  从webserver 	flume	 hdfs
  设计目标：可靠性 扩展性 管理性  组成agent来完成
业界同类产品的对比
*Flume	cloudera/Apache	Java
Scribe	Facebook	C/C++		不再维护
Chukwa	Yahoo/Apache 	  Java	不再维护
Kafka：仅仅是分布式缓冲
Fluented: Ruby
*Logstash: ELK(ElaticSearch Kibana)
Cloudera	FlumeOG
flume-728	Flume-NG   ==> Apache
1.6 1.7 1.8
Flume架构和核心组件
1)  source 	收集   收集数据
2）channel
	Memory    File	Kafka	JDBC
	类似与消息传递的缓冲池
	减少与 sink的交互次数
3）sink 
Avro Source AvroSink 配对使用
多个agent的flow

Flume 安装
1.java 1.7
2.足够的内存
3.足够的磁盘空间
4.目录的权限


解压flume
flume-env.sh 中配置java的目录
使用flume的关键就是写配置文件
配置source
配置channel
配置sink
把组件都连起来



# example.conf: A single-node Flume configuration

# Name the components on this agent
#a1 agent的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


bin/flume-ng agent 
--conf conf        	配置文件夹
 --conf-file example.conf  配置文件
--name a1     文件名称
-Dflume.root.logger=INFO,console
event 是flume中传输日志的最小单元

source
需求二
exec source  + memory + logger sink
 # example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/zw/hadoop
a1.sources.r1.shell = /bin/bash -c

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
~                    


需求3
exec source + memory+
把机器A上的日志同步到机器B上
机器A： exec source ---> avro sink
avro sink 要指定到什么地方去
机器B:
avro source ------> loogersink

exec-memory-avro.conf

exec-memory-avro.sources = exec-source
exec-memory-avro.sinks = avro-sink
exec-memory-avro.channels = memory-channel

# Describe/configure the source
exec-memory-avro.sources.exec-source.type = exec
exec-memory-avro.sources.exec-source.command = tail -F /home/zw/hadoop
exec-memory-avro.sources.exec-source.shell = /bin/bash -c

# Describe the sink
exec-memory-avro.sinks.avro-sink.type = avro
exec-memory-avro.sinks.avro-sink.hostname = 192.168.56.112
exec-memory-avro.sinks.avro-sink.port = 44444

# Use a channel which buffers events in memory
exec-memory-avro.channels.memory-channel.type = memory
exec-memory-avro.channels.memory-channel.capacity = 1000
exec-memory-avro.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
exec-memory-avro.sources.exec-source.channels = c1
exec-memory-avro.sinks. memory-channel.channel = c1










avro-memory-logger.conf

avro-memory-logger.sources = avro-source
avro-memory-loggersinks = logger-sink
avro-memory-logger.channels = memory-channel

# Describe/configure the source
avro-memory-logger.avro-source.type = avro
avro-memory-logger.avro-source.bind = 127.0.0.1
avro-memory-logger.avro-source.port = 44444

# Describe the sink
avro-memory-logger.sinks.logger-sink.type = logger

# Use a channel which buffers events in memory
avro-memory-logger.channels.memory-channel.type = memory
avro-memory-loggerchannels.memory-channel.capacity = 1000
avro-memory-logger.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
avro-memory-logger.sources.avro-source.channels = c1
avro-memory-logger.sinks. logger-sink.channel = c1

avro 
日志收集过程 机器A上行为日志记录到access.log里
avro source把日志放入kafka中


Kafka 分布式消息队列
和消息系统类似 
消息中间件
生产者	消费者 数据流
生产一个 消费一个 
消费者 机器故障 数据丢失 
数据源数据多 数据丢失   
kafka 是起一个缓冲的作用 消息
篮子 Kafka
多准备几个篮子 ===== 相当于Kafka的扩容 
Kafka就是个篮子

Kafka架构
producer 生产者 生产馒头
consumer 消费者 吃馒头
broker 篮子  
topic 主题 给馒头一个标签 topic 表明是给谁的

单结点单broker
单结点多broker
多结点多broker

单结点单broker
kafka server.properties
brokerid=0 broker的编号不能重复
listeners
host.name
log.dirs 注意不能是tmp目录 会清空
启动目录
bin/kafka-server-start.sh config/server.properties
创建topic zk
bin/kafka-create-topic.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  --topic hello_topic
查看所有topic
 bin/kafka-list-topic.sh --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181
发送消息 broker 
bin/kafka-console-producer.sh --broker-list master:9092 --topic hello_topic
消费消息
bin/kafka-console-consumer.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181 --topic hello_topic --from-beginning
官方文档上是错的 不是brokerlist
--frombegining 表示从头开始消费

单结点多broker启动
server-1.properties
server-2.properties
server-3.properties
bin/kafka-server-start.sh config/server-1.properties &
bin/kafka-server-start.sh config/server-2.properties &
bin/kafka-server-start.sh config/server-3.properties &

create

bin/kafka-create-topic.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  --topic muti_topic --replica  3


bin/kafka-console-producer.sh --broker-list master:9093,master:9094,master:9095  --topic muti_topic

bin/kafka-console-consumer.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181 --topic muti_topic 


kafka的容错性的测试

只要有一个副本正常 就是可以正常传输的 
只要有一个replicationFactor 正常的 就可以正常操作 

kafkajavaapi
producer.config
required.acks 0 1 -1  不等待  记录到本地  等待回值

0.9.0 
bin/kafka-topics.sh --create --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181   --replication-factor 1 --partitions 1 --topic test

 bin/kafka-topics.sh --list --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181

 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

  bin/kafka-console-consumer.sh --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  --topic hello_topic --from-beginning

flume+kafka

webserver + avrosource -> memeorychannel -> avro sink
+avrosource -> memory -> logger sink

sink kafka 生产者
kafka consumer 就是ok的

整合flume kafka完成采集 

bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
#a1.sources.r1.type = netcat
#a1.sources.r1.bind = localhost
#a1.sources.r1.port = 44444

a1.sources.r1.type = exec
a1.sources.r1.command = tail -F ~/data/data.log
a1.sources.r1.shell = /bin/bash -c

# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.brokerList = 192.168.56.111:9092
a1.sinks.k1.topic = hello_topic
a1.sinks.k1.batchSize = 5
a1.sinks.k1.requiredAcks = 1

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

spark using source code to compile 
the same version to hadoop



Spark Streaming is an extension of the core Spark API that allows enables scalable, high-throughput, fault-tolerant stream processing of live data streams.
scalable, 
high-throughput, 
fault-tolerant
spark-streaming
将不同的数据源经过spark streaming 处理之后输出到外部文件系统

特点
	低延迟
	能够从错误中高效恢复
	能够运行在成百上千的结点
	能够将批处理 机器学习	图计算等子框架和Spark Streaming 综合起来使用

tachyon
sparkcore

spark streaming
graphX
Mllib
SparkSql
Spark Streaming 不需要独立安装 不需要独立安装 ？
One stack to rule them all 一栈式解决

应用场景
电商行业的实时推荐 实时处理  相应商品的转换率
对于防火墙的处理
对错误日志 对错误进行统计分析

kafkaStream.transform{
bacthRDD
}
RDD
kafkaStream.map{}
对实时流数据使用sql进行查询
伯克利实验室 
Stable realse  
对于功能出现的时间点 面试
spark-submit
spark-shell
使用spark-submit来提交应用程序的脚本
./spark-submit --master local[2]  --class  org.apache.spark.examples.streaming.NetworkWordCount --name NetworkWordCount  /home/zw/hadoop/spark/spark-2.2.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.2.0.jar localhost 9999

local后的数字一定要大于1

使用sparkshell
./spark-shell --master local[2] 

    val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    val ssc = new StreamingContext(sc, Seconds(1))
    val lines = ssc.socketTextStream(localhost,9999)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
ssc.awaitTermination()

SparkStreaming
def this(sparkContext: SparkContext, batchDuration: Duration) = {
    this(sparkContext, null, batchDuration)
  }
local[*]表示运行在本地
单例测试 unit测试 
batch interval 必须设置 基于业务逻辑的延迟性以及集群可用的资源的情况来设置
StreamingContext 
1.拿到输入的源
2.做一些必要的计算
3.StreamingContext的start方法
4.手动停止 或者error信息停止
tips
1.一个Streaming Context启动不能添加新的计算
2.stop之后就不能再重启
3.一个StreamingContext就不能在启动了
4.stop 可以分别停止StreamingContext和SparkContext

Discretized Streams DStreams
最基本的抽象 持续化的数据流 1.从源头 2.从其他DStream
一个DStream代表着一系列的 不间断的RDD
根据我们的时间间隔来进行拆分 
每一个RDD包含着这一批次里面的数据 
对一个DStream操作算子 
入flatmap操作 相当于对DStream中每一个RDD做相同的flatmap操作
因为每一个DSTREAM是由不同批次的RDD构成的

Input Dstreaming and Reciever

Input Destream 从源头接受过来的DStream 从端口上接受过来的 每一个InputDStream and Recivier
回关联一个Reciever 存在spark内存里 除了fileSystem 比如说 hdfs
基础数据源 从数据库中读取 高级数据源 kafka/flume
reciver表示从数据源接受数据
不要用local或者local[1] local[1]表示只有一个线程能够使用 使用reciver便没有其他线程可以使用
transformation
用法和SparkCore的用法十分类似
SparkStreaming InputStreaming 转换数据结果 
实时同步到文件系统里去

SparkStreaming处理socket的数据
