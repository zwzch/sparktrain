需求：统计主站每个指定课程访问的客户端 地域信息的分布
地域 ip转换 spark sql项目实战
 useragent 获取  hadoop基础课程
离线的方式进行统计

实现步骤 
	课程编号 ip信息 useragent
	mapreduce 和spark
 项目架构
	日志收集 flume
	离线分析 mapreduce/spark
	统计结果图形化展示
 问题     小时级别   5分钟   1分钟 秒级别
mapreduce 的时效性不高 mapreduce 启动和销毁进程回浪费时间
离线的批处理 spark sparksql map/reduce
解决 ===> 实时流处理的框架
时效性高  业务营销
数据量大  以实时方式处理大的数据 保证数据的准确性 扩展性好的系统 生命周期短的业务 
从价值密度低的数据提取有价值数据 
实时计算  流式计算  实时流式计算
数据来源
	hdfs 历史数据 数据量比较大
	实时：消息队列（kafka）实时新增/修改记录过来的某一笔数据
处理过程：
	离线 map+reduce 慢


SparkStreaming
Sparkconf --> Streamingcontxt(conf, 切分批次)

	实时 spark（DStream/SS）
处理速度
	慢
	快速
进程
	离线： 启动销毁
	实时： 7×24



apache storm 真正的实时框架
apache spark streaming 微批处理 按照时间间隔缩小 1秒 2秒
IBM Strea
Yahoo S4
LinkedIn Kafka streaming platform
Flink

WEB/APP            WebServer(tomcat nginx)access.log      flume(分布式日志收集框架)
加kafka是为了抗住高峰时的日志压力       spark/storm          rdbms/nosql           可视化展示
实际应用 
电信行业流量计算的实时计算 防止流量超标 手机频繁网络传输 有流量陷阱的软件
电商行业 1.大屏幕展示 2.推测属于哪一个用户 分析出关联产品 





Flume
network divices	operating system	web services		Applications
其他的server移动到hadoop上
1.采用脚本直接传输 cp到集群的机器上 hadoop fs -put
   问题 没有办法做到很好的监控  实效性会低   文件直接传输io开销大  怎么做容错和负载均衡
   脚本回出现问题
2.Flume 
   Flume的使用主要是写配置文件  从A->B
  分布式 高可靠  
  collect收集 	aggreating聚合		moving 移动
 在线实时的分析应用 指定webserver 从webserver
  source + channel + sink
  从webserver 	flume	 hdfs
  设计目标：可靠性 扩展性 管理性  组成agent来完成
业界同类产品的对比
*Flume	cloudera/Apache	Java
Scribe	Facebook	C/C++		不再维护
Chukwa	Yahoo/Apache 	  Java	不再维护
Kafka：仅仅是分布式缓冲
Fluented: Ruby
*Logstash: ELK(ElaticSearch Kibana)
Cloudera	FlumeOG
flume-728	Flume-NG   ==> Apache
1.6 1.7 1.8
Flume架构和核心组件
1)  source 	收集   收集数据
2）channel
	Memory    File	Kafka	JDBC
	类似与消息传递的缓冲池
	减少与 sink的交互次数
3）sink 
Avro Source AvroSink 配对使用
多个agent的flow

Flume 安装
1.java 1.7
2.足够的内存
3.足够的磁盘空间
4.目录的权限


解压flume
flume-env.sh 中配置java的目录
使用flume的关键就是写配置文件
配置source
配置channel
配置sink
把组件都连起来



# example.conf: A single-node Flume configuration

# Name the components on this agent
#a1 agent的名称
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1


bin/flume-ng agent 
--conf conf        	配置文件夹
 --conf-file example.conf  配置文件
--name a1     文件名称
-Dflume.root.logger=INFO,console
event 是flume中传输日志的最小单元

source
需求二
exec source  + memory + logger sink
 # example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/zw/hadoop
a1.sources.r1.shell = /bin/bash -c

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
~                    


需求3
exec source + memory+
把机器A上的日志同步到机器B上
机器A： exec source ---> avro sink
avro sink 要指定到什么地方去
机器B:
avro source ------> loogersink

exec-memory-avro.conf

exec-memory-avro.sources = exec-source
exec-memory-avro.sinks = avro-sink
exec-memory-avro.channels = memory-channel

# Describe/configure the source
exec-memory-avro.sources.exec-source.type = exec
exec-memory-avro.sources.exec-source.command = tail -F /home/zw/hadoop
exec-memory-avro.sources.exec-source.shell = /bin/bash -c

# Describe the sink
exec-memory-avro.sinks.avro-sink.type = avro
exec-memory-avro.sinks.avro-sink.hostname = 192.168.56.112
exec-memory-avro.sinks.avro-sink.port = 44444

# Use a channel which buffers events in memory
exec-memory-avro.channels.memory-channel.type = memory
exec-memory-avro.channels.memory-channel.capacity = 1000
exec-memory-avro.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
exec-memory-avro.sources.exec-source.channels = c1
exec-memory-avro.sinks. memory-channel.channel = c1










avro-memory-logger.conf

avro-memory-logger.sources = avro-source
avro-memory-loggersinks = logger-sink
avro-memory-logger.channels = memory-channel

# Describe/configure the source
avro-memory-logger.avro-source.type = avro
avro-memory-logger.avro-source.bind = 127.0.0.1
avro-memory-logger.avro-source.port = 44444

# Describe the sink
avro-memory-logger.sinks.logger-sink.type = logger

# Use a channel which buffers events in memory
avro-memory-logger.channels.memory-channel.type = memory
avro-memory-loggerchannels.memory-channel.capacity = 1000
avro-memory-logger.channels.memory-channel.transactionCapacity = 100

# Bind the source and sink to the channel
avro-memory-logger.sources.avro-source.channels = c1
avro-memory-logger.sinks. logger-sink.channel = c1

avro 
日志收集过程 机器A上行为日志记录到access.log里
avro source把日志放入kafka中


Kafka 分布式消息队列
和消息系统类似 
消息中间件
生产者	消费者 数据流
生产一个 消费一个 
消费者 机器故障 数据丢失 
数据源数据多 数据丢失   
kafka 是起一个缓冲的作用 消息
篮子 Kafka
多准备几个篮子 ===== 相当于Kafka的扩容 
Kafka就是个篮子

Kafka架构
producer 生产者 生产馒头
consumer 消费者 吃馒头
broker 篮子  
topic 主题 给馒头一个标签 topic 表明是给谁的

单结点单broker
单结点多broker
多结点多broker

单结点单broker
kafka server.properties
brokerid=0 broker的编号不能重复
listeners
host.name
log.dirs 注意不能是tmp目录 会清空
启动目录
bin/kafka-server-start.sh config/server.properties
创建topic zk
bin/kafka-create-topic.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  --topic hello_topic
查看所有topic
 bin/kafka-list-topic.sh --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181
发送消息 broker 
bin/kafka-console-producer.sh --broker-list master:9092 --topic hello_topic
消费消息
bin/kafka-console-consumer.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181 --topic hello_topic --from-beginning
官方文档上是错的 不是brokerlist
--frombegining 表示从头开始消费

单结点多broker启动
server-1.properties
server-2.properties
server-3.properties
bin/kafka-server-start.sh config/server-1.properties &
bin/kafka-server-start.sh config/server-2.properties &
bin/kafka-server-start.sh config/server-3.properties &

create

bin/kafka-create-topic.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  --topic muti_topic --replica  3


bin/kafka-console-producer.sh --broker-list master:9093,master:9094,master:9095  --topic muti_topic

bin/kafka-console-consumer.sh  --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181 --topic muti_topic 


kafka的容错性的测试

只要有一个副本正常 就是可以正常传输的 
只要有一个replicationFactor 正常的 就可以正常操作 

kafkajavaapi
producer.config
required.acks 0 1 -1  不等待  记录到本地  等待回值

0.9.0 
bin/kafka-topics.sh --create --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181   --replication-factor 1 --partitions 1 --topic test

 bin/kafka-topics.sh --list --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181

 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

  bin/kafka-console-consumer.sh --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  --topic hello_topic --from-beginning

flume+kafka

webserver + avrosource -> memeorychannel -> avro sink
+avrosource -> memory -> logger sink

sink kafka 生产者
kafka consumer 就是ok的

整合flume kafka完成采集 

bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console

# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
#a1.sources.r1.type = netcat
#a1.sources.r1.bind = localhost
#a1.sources.r1.port = 44444

a1.sources.r1.type = exec
a1.sources.r1.command = tail -F ~/data/data.log
a1.sources.r1.shell = /bin/bash -c

# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.brokerList = 192.168.56.111:9092
a1.sinks.k1.topic = hello_topic
a1.sinks.k1.batchSize = 5
a1.sinks.k1.requiredAcks = 1

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

spark using source code to compile 
the same version to hadoop



Spark Streaming is an extension of the core Spark API that allows enables scalable, high-throughput, fault-tolerant stream processing of live data streams.
scalable, 
high-throughput, 
fault-tolerant
spark-streaming
将不同的数据源经过spark streaming 处理之后输出到外部文件系统

特点
	低延迟
	能够从错误中高效恢复
	能够运行在成百上千的结点
	能够将批处理 机器学习	图计算等子框架和Spark Streaming 综合起来使用

tachyon
sparkcore

spark streaming
graphX
Mllib
SparkSql
Spark Streaming 不需要独立安装 不需要独立安装 ？
One stack to rule them all 一栈式解决

应用场景
电商行业的实时推荐 实时处理  相应商品的转换率
对于防火墙的处理
对错误日志 对错误进行统计分析

kafkaStream.transform{
bacthRDD
}
RDD
kafkaStream.map{}
对实时流数据使用sql进行查询
伯克利实验室 
Stable realse  
对于功能出现的时间点 面试
spark-submit
spark-shell
使用spark-submit来提交应用程序的脚本
./spark-submit --master local[2]  --class  org.apache.spark.examples.streaming.NetworkWordCount --name NetworkWordCount  /home/zw/hadoop/spark/spark-2.2.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.2.0.jar localhost 9999

local后的数字一定要大于1

使用sparkshell
./spark-shell --master local[2] 

    val sparkConf = new SparkConf().setAppName("NetworkWordCount")
    val ssc = new StreamingContext(sc, Seconds(1))
    val lines = ssc.socketTextStream(localhost,9999)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
ssc.awaitTermination()

SparkStreaming
def this(sparkContext: SparkContext, batchDuration: Duration) = {
    this(sparkContext, null, batchDuration)
  }
local[*]表示运行在本地
单例测试 unit测试 
batch interval 必须设置 基于业务逻辑的延迟性以及集群可用的资源的情况来设置
StreamingContext 
1.拿到输入的源
2.做一些必要的计算
3.StreamingContext的start方法
4.手动停止 或者error信息停止
tips
1.一个Streaming Context启动不能添加新的计算
2.stop之后就不能再重启
3.一个StreamingContext就不能在启动了
4.stop 可以分别停止StreamingContext和SparkContext

Discretized Streams DStreams
最基本的抽象 持续化的数据流 1.从源头 2.从其他DStream
一个DStream代表着一系列的 不间断的RDD
根据我们的时间间隔来进行拆分 
每一个RDD包含着这一批次里面的数据 
对一个DStream操作算子 
入flatmap操作 相当于对DStream中每一个RDD做相同的flatmap操作
因为每一个DSTREAM是由不同批次的RDD构成的

Input Dstreaming and Reciever

Input Destream 从源头接受过来的DStream 从端口上接受过来的 每一个InputDStream and Recivier
回关联一个Reciever 存在spark内存里 除了fileSystem 比如说 hdfs
基础数据源 从数据库中读取 高级数据源 kafka/flume
reciver表示从数据源接受数据
不要用local或者local[1] local[1]表示只有一个线程能够使用 使用reciver便没有其他线程可以使用
transformation
用法和SparkCore的用法十分类似
SparkStreaming InputStreaming 转换数据结果 
实时同步到文件系统里去

SparkStreaming处理socket的数据
spark-submit --master local[2]  --class  NetworkWordCount --name NetworkWordCount  /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar

SocketTextStream 通过监控端口
FileStream SparkStreaming可以监控文件夹
1.递归目录不支持
2.文件格式必须一样
3.文件必须是原子性的 处理过后的数据不会再被处理
4.不需要使用reciver

1.带状态的算子 UpdateStateBykey
  计算到当前未止出现的单词个数写到mysql中
  基于window的统计 黑名单过滤
  Spark Streaming + Spark Sql
生产环境中 checkpoint写到hdfs中
以DStream中的数据进行按key做reduce操作，然后对各个批次的数据进行累加
在有新的数据信息进入或更新时。能够让用户保持想要的不论什么状。使用这个功能须要完毕两步：
1) 定义状态：能够是随意数据类型
2) 定义状态更新函数：用一个函数指定怎样使用先前的状态。从输入流中的新值更新状态。
对于有状态操作，要不断的把当前和历史的时间切片的RDD累加计算，随着时间的流失，计算的数据规模会变得越来越大。
新的东西进来 每一个都会 调用
有metaCheckPoint 和 dataCheckPoint
spark-submit --master local[2]  --class  StatefulWordCount --name StatefulWordCount  /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar

一个Streaming应用程序要求7天24小时不间断运行，因此必须适应各种导致应用程序失败的场景。Spark Streaming的检查点具有容错机制，有足够的信息能够支持故障恢复。支持两种数据类型的检查点：元数据检查点和数据检查点。

（1）元数据检查点，在类似HDFS的容错存储上，保存Streaming计算信息。这种检查点用来恢复运行Streaming应用程序失败的Driver进程。

（2）数据检查点，在进行跨越多个批次合并数据的有状态操作时尤其重要。在这种转换操作情况下，依赖前一批次的RDD生成新的RDD，随着时间不断增加，RDD依赖链的长度也在增加，为了避免这种无限增加恢复时间的情况，通过周期检查将转换RDD的中间状态进行可靠存储，借以切断无限增加的依赖。使用有状态的转换，如果updateStateByKey或者reduceByKeyAndWindow在应用程序中使用，那么需要提供检查点路径，对RDD进行周期性检查。

元数据检查点主要用来恢复失败的Driver进程，而数据检查点主要用来恢复有状态的转换操作。无论是Driver失败，还是Worker失败，这种检查点机制都能快速恢复。许多Spark Streaming都是使用检查点方式。但是简单的Streaming应用程序，不包含状态转换操作不能运行检查点；从Driver程序故障中恢复可能会造成一些收到没有处理的数据丢失。

为了让一个Spark Streaming程序能够被恢复，需要启用检查点，必须设置一个容错的、可靠的文件系统（如HDFS、S3等）路径保存检查点信息，同时设置时间间隔。

streamingContext.checkpoint(checkpointDirectory)//checkpointDirectory是一个文件系统路径（最好是一个可靠的比如hdfs://....）

dstream.checkpoint(checkpointInterval)//设置时间间隔

当程序第一次启动时，创建一个新的StreamingContext，接着创建所有的数据流，然后再调用start()方法。

//定义一个创建并设置StreamingContext的函数

def functionToCreateContext(): StreamingContext = {

val ssc = new StreamingContext(...)               //创建StreamingContext实例

val DsSream = ssc.socketTextStream(...)      //创建DStream

...

ssc.checkpoint(checkpointDirectory)           //设置检查点机制

ssc

}

//从检查点数据重建或者新建一个StreamingContext

val context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreate-Context_)

//在context需要做额外的设置完成，不考虑是否被启动或重新启动

context. ...

//启动context

context.start()

context.awaitTermination()

通过使用getOrCreate创建StreamingContext。

当程序因为异常重启时，如果检查点路径存在，则context将从检查点数据中重建。如果检查点目录不存在（首次运行），将会调用functionToCreateContext函数新建context函数新建context，并设置DStream。

但是，Streaming需要保存中间数据到容错存储系统，这个策略会引入存储开销，进而可能会导致相应的批处理时间变长，因此，检查点的时间间隔需要精心设置。采取小批次时，每批次检查点可以显著减少操作的吞吐量；相反，检查点太少可能会导致每批次任务大小的增加。对于RDD检查点的有状态转换操作，其检查点间隔默认设置成DStream的滑动间隔的5~10倍。

故障恢复可以使用Spark的Standalone模式自动完成，该模式允许任何Spark应用程序的Driver在集群内启动，并在失败时重启。而对于YARN或Mesos这样的部署环境，则必须通过其他的机制重启Driver。

需求 将统计结果存入mysql中
create table wordcount(
word varchar(50) default null,
wordcount int(10) default null
)
Task not serialzition
spark-submit --jars /home/zw/桌面/newiotest/libb/mysql-connector-java-5.1.18.jar  --master local[2]  --class  ForeachRDDApp --name ForeachRDDApp  /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar
val sql = "insert into wordcount(word, wordcount) values('" + record._1 + "','"+record._2 + "')"
这个sql存在问题
对已有的数据没有做更新 而是所有的数据都是insert 在插入数据前判断是否存在 存在就update
工作中使用 Hbase/Redis 自己更新
连接需要改变为连接池

SparkStream window
window ： 定时的进行一个时间段内的数据处理
windows 就是时间段的长度 窗口操作经过多久处理一次  隔两个窗口再计算一个窗口
 windowlength 窗口长度
 sliding interval 窗口的间隔
 这两个参数和batch sizes 是一个整数倍数的关系
 每隔多久计算某个范围内的数据
 每隔10秒计算前10分钟的wc
 每隔sliding interval统计window length的值
 3个参数 操作 窗口大小 间隔大小
reductByKeyAndWindow 仅仅是算子不同

黑名单过滤
1.transform 2.join
需求
访问日志 ==>DStream
20180808,zs
20180808,ls
20180808,ww
    ===> (zs:20180808,zs)
黑名单列表：==>RDD
zs
ls
    ====> (zs:true)(ls:true)

spark-submit --jars /home/zw/桌面/newiotest/libb/mysql-connector-java-5.1.18.jar  --master local[2]  --class  TransformApp   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar

20160410,zs
20160410,ls
20160410,ww
根据需求 left 保证左边的数据都是全的
transform + leftjoin 把不要的东西过滤掉
outleftjoin
SparkSteaming + Flume
第一种方式
Flume-style Push-based Approach
flume avro的形式
push形式 spark先启动
FlumeUtils.createStream(streamingContext, [chosen machine's hostname], [chosen port])

# list the sources, sinks and channels in the agent
agent_foo.sources = netcat-source
agent_foo.sinks = avro-sink
agent_foo.channels = memory-channel

# set channels for source
agent_foo.sources.netcat-source.type = netcat
agent_foo.sources.netcat-source.bind = 127.0.0.1
agent_foo.sources.netcat-source.type = 44444

agent_foo.sinks.avro-sink.type = avro
agent_foo.sinks.avro-sink.type = 127.0.0.1
agent_foo.sinks.avro-sink.type = 41414

agent_foo.channels.memory-channel.type = memory

agent_foo.sources.netcat-source.channels = memory-channel
agent_foo.sinks.avro-sink.channel = memory-channel


<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-flume-sink_2.11</artifactId>
    <version>2.2.0</version>
</dependency>
spark-submit --jars  /home/zw/.m2/repository/org/apache/spark/spark-streaming-flume_2.11/2.2.0/spark-streaming-flume_2.11-2.2.0.jar,/home/zw/.m2/repository/org/apache/flume/flume-ng-core/1.6.0/flume-ng-core-1.6.0.jar,/home/zw/.m2/repository/org/apache/flume/flume-ng-sdk/1.6.0/flume-ng-sdk-1.6.0.jar --master local[2]  --class  FlumePushWordCount   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar
bin/flume-ng agent --conf conf --conf-file example.conf --name agent_foo -Dflume.root.logger=INFO,console
1)启动sparkstreaming
2）启动flumeagent
3）通过telnet
--packages 使用maven的从网络上下载相关的依赖包
Pull-based Approach using a Custom Sink

SparkStreaming Push Based Flume

第二种方式
自定义的sink
有事务保证 有副本
Flume pushes data into the sink, and the data stays buffered.
Spark Streaming uses a reliable Flume receiver and transactions to pull data from the sink. Transactions succeed only after data is received and replicated by Spark Streaming.

agent_foo.sources = netcat-source
agent_foo.sinks = spark
agent_foo.channels = memory-channel

# set channels for source
agent_foo.sources.netcat-source.type = netcat
agent_foo.sources.netcat-source.bind = 127.0.0.1
agent_foo.sources.netcat-source.type = 44444

agent_foo.sinks = spark
agent_foo.sinks.spark.type = org.apache.spark.streaming.flume.sink.SparkSink
agent_foo.sinks.spark.hostname = 127.0.0.1
agent_foo.sinks.spark.port = 41414



agent_foo.channels.memory-channel.type = memory

agent_foo.sources.netcat-source.channels = memory-channel
agent_foo.sinks.spark.channel = memory-channel

先启动flume 然后启动spark
mvn -CleanPackage -d
第二种是有备份的 更可靠的方式


SparkStreaming Kafka
0.8 稳定的
0.10的集成是不兼容以前版本的 实验性的
Receiver-based 基于reciver的 出现故障可能丢失数据
reciver的高级api操作 存储的哦spark executor
WAL write ahead log 在spark1.2之后推出的 预写日志 防止数据丢失
spark 关键功能的时间 面试会问
import KafkaUtils
可以采用并行的机制
WAL机制需要有一个多副本的文件系统
生产环境一般使用 --jars

spark-submit --jars  /home/zw/.m2/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar  --master local[2]  --class  KafkaReciverWordCount   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  test  1

sparkStreaming 监控界面 默认4040 reciver job是一直存在的 挂掉了就没有接受数据的

DirectApproach 基于director的
通过kafka中偏移量读取来判定如何读取
是DirectStream
1.不需要reciver
2.不需要wal的模式 就可以达到0副本的丢失
3.能够满足只执行一次 exactly-once 一次语义

缺点 需要自己周期性的向zookeeper中更新数据

spark-submit --jars  /home/zw/.m2/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar,/home/zw/下载/kafka_2.11-0.9.0.0/libs/kafka_2.11-0.9.0.0.jar,/home/zw/下载/kafka_2.11-0.9.0.0/libs/kafka-clients-0.9.0.0.jar  --master local[2]  --class  KafkaDirectWordCount   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar 192.168.56.111:9092 hello_topic

bin/kafka-server-start.sh -daemon  config/server.properties

bin/kafka-topics.sh --list --zookeeper 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181

bin/kafka-console-producer.sh --broker-list localhost:9092 --topic streaming_topic


在生产上
1）要把log4j的生成类需要打成jar包
2)sparkStreaming 需要达成jar包 选择运行环境 运行模式 local/yarn/standalone/mesos
在生产上流处理流程是一样的 区别在于业务逻辑和复杂性

用户的行为日志 访问 搜索 点击 关联用户 增多维度

记录原因
网站页面的访问量
网站的黏性
使用系统 对用户打一个标签 对用户进行推荐 进行分析

用户行为日志内容
ip 时间 区域 客户端 模块的类别 跳转的链接地址 不同的业务是不同的

accesslog 是网站的眼睛 访问者的来源
          网站布局的是否合理 推荐课程 销量比较好的课程
          分配预算 BI出报表 互联网的
          让直播不卡顿 进来送礼物有日志 弹幕收集日志 统计日志 关联收益
          处理日志的规模变大 机器是否能抗的住 就会产生问题

linux corntab表达式
tool.lu/crontab
crontab -e
*/1 * * * * /home/zw/PycharmProjects/Demo1/runLogs.sh

对接python日志产生器到Flume
access.log ====> 控制台输出
    exec
    memory
    logger

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /Users/guoxingyu/Documents/work/spark/sql/generate_log/logs/access.log
a1.sources.r1.shell = /bin/sh -c

# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.topic = streamingtopic
a1.sinks.k1.brokerList = localhost:9092
a1.sinks.k1.requiredAcks = 1
a1.sinks.k1.batchSize = 20

# Use a channel which buffers events in memory
a1.channels.c1.type = memory

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

bin/flume-ng agent --conf conf --conf-file  --name a1 -Dflume.root.logger=INFO,console

日志 ===> flume====>kafka
    启动zk
    启动kafka
    修改flume配置文件

spark-submit --jars  /home/zw/.m2/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar,/home/zw/下载/kafka_2.11-0.9.0.0/libs/kafka_2.11-0.9.0.0.jar,/home/zw/下载/kafka_2.11-0.9.0.0/libs/kafka-clients-0.9.0.0.jar  --master local[2]  --class  project.StatStreamingApp   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181  streaming_topic  1

spark-submit --jars  /home/zw/.m2/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar  --master local[2]  --class  project.spark.StatStreamingApp   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181 test streaming_topic  1


数据清洗 -----> 从原始日志中解析出需要的日志
功能1 ： 今天到现在为止实战课程的访问量
    Spark Streaming把统计结果写入到数据库里
    可视化前端根据 yyyyMMdd courseid 把数据库的统计结果展示出来
    RDBMS
        现有的拿出来 + 下一个批次的统计结构
    NoSQL：
        HBSASE 一个API就能搞定
        clickout + 统计结果

HBase 表设计
    create 'course_clickcount', 'info'
RowKey 设计
    day_courseid

如何使用scala来操作Hbase
spark-submit --jars  /home/zw/.m2/repository/org/apache/spark/spark-streaming-kafka-0-8_2.11/2.2.0/spark-streaming-kafka-0-8_2.11-2.2.0.jar,/home/zw/IdeaProjects/sparktrain/lib/hbase_lib/hbase-client-0.98.12-hadoop2.jar,/home/zw/hadoop/spark/spark-2.2.0-bin-hadoop2.6/jars/hadoop-common-2.6.5.jar,/home/zw/IdeaProjects/sparktrain/lib/hbase_lib/hbase-common-0.98.12-hadoop2.jar,/home/zw/IdeaProjects/sparktrain/lib/hbase_lib/hbase-protocol-0.98.12-hadoop2.jar,/home/zw/IdeaProjects/sparktrain/lib/hbase_lib/htrace-core-2.04.jar  --master local[2]  --class  project.spark.StatStreamingApp   /home/zw/IdeaProjects/sparktrain/out/artifacts/sparktrain_jar/sparktrain.jar 192.168.56.114:2181,192.168.56.112:2181,192.168.56.113:2181 test streaming_topic  1

功能1 + 从搜索引擎引流过来的
Hbase表设计
    create 'crease_serach_clickcount' 'info'
    设计rowkey
    20171111+search+1

SpringBoot
Echarts 动态 静态
DataV

为什么需要可视化
理解数据意义的过程
可视化可以提高效率
SpringBoot
采用了生产级别的Spring程序  约定大于配置
少写很多配置文件！！！
sprintBoot动态获取hbase的数据
1.代码中写死
2.在页面上放一个时间插件